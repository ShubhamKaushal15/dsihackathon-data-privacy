{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from extensions.schemagen import schemagen\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"yellow_cab_data\", \"yellow_cab_data.csv\"))\n",
    "\"\"\"\n",
    "One hot encodes a column in a dataframe.\n",
    "@param pd.DataFrame df: the dataframe\n",
    "@param str colname: the name of the column to be one-hot encoded\n",
    "\"\"\"\n",
    "def one_hot(df: pd.DataFrame, colname: str):\n",
    "    hot = pd.get_dummies(df[colname], prefix=colname)\n",
    "    df.drop(colname, axis=1, inplace=True)\n",
    "    return df.join(hot)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Preprocesses data by dropping columns we don't want and one-hot encoding\n",
    "specified categorical ones.\n",
    "@param pd.DataFrame data: the data to preprocess\n",
    "@param list drop: column names to drop\n",
    "@param list onehot: column names to onehot encode\n",
    "\"\"\"\n",
    "def preprocess_data(data: pd.DataFrame, drop=[], onehot=[]):\n",
    "    df = data.copy()\n",
    "    df = df.dropna(axis=0)\n",
    "    # Drop features we don't want\n",
    "    for feat in drop:\n",
    "        df.drop(feat, axis=1, inplace=True)\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    for feat in onehot:\n",
    "        df = one_hot(df, feat)\n",
    "\n",
    "    # Encode store_and_fwd_flag\n",
    "    store = (df[\"store_and_fwd_flag\"] == \"Y\").astype(int)\n",
    "    df[\"store_and_fwd_flag\"] = store\n",
    "\n",
    "    df = df[df[\"tip_amount\"] > 0]\n",
    "\n",
    "    for col in ['extra', 'mta_tax', 'improvement_surcharge', 'tolls_amount', 'congestion_surcharge', 'airport_fee']:\n",
    "        df[col] = abs(df[col])\n",
    "\n",
    "    df = df[(np.abs(stats.zscore(df.tip_amount))) < 3]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df, \n",
    "                drop = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \n",
    "                        \"total_amount\", \"payment_type\", \"fare_amount\", \n",
    "                        \"trip_distance\"]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('tip_amount', axis = 1), df['tip_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "train_data.to_csv(os.path.join(\"yellow_cab_data\", \"processed_train.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 20\n",
    "EPSILON = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_generator = schemagen.SchemaGenerator()\n",
    "input_file = os.path.join(\"yellow_cab_data\", \"processed_train.csv\")\n",
    "schema_generator.read_and_parse_csv(input_csv_file = input_file,\n",
    "                                    categorical_columns = ['PULocationID', 'DOLocationID'],\n",
    "                                    num_bins = NUM_BINS)\n",
    "parameters_json = schema_generator.get_parameters_json()\n",
    "\n",
    "with open(os.path.join(\"yellow_cab_data\", \"parameters.json\"), 'w') as fp:\n",
    "    json.dump(parameters_json, fp, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shubham/private-pgm/src/mbi/__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n",
      "  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n",
      "Transform complete\n",
      "Dataset and domain file look good!\n",
      "Domain check complete\n",
      "/home/shubham/private-pgm/src/mbi/__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n",
      "  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n",
      "\n",
      "Measuring ('PULocationID',), L2 sensitivity 1.000000\n",
      "Measuring ('store_and_fwd_flag',), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID',), L2 sensitivity 1.000000\n",
      "Measuring ('congestion_surcharge',), L2 sensitivity 1.000000\n",
      "Measuring ('tip_amount',), L2 sensitivity 1.000000\n",
      "Measuring ('mta_tax',), L2 sensitivity 1.000000\n",
      "Measuring ('extra',), L2 sensitivity 1.000000\n",
      "Measuring ('tolls_amount',), L2 sensitivity 1.000000\n",
      "Measuring ('RatecodeID',), L2 sensitivity 1.000000\n",
      "Measuring ('airport_fee',), L2 sensitivity 1.000000\n",
      "Measuring ('improvement_surcharge',), L2 sensitivity 1.000000\n",
      "Measuring ('passenger_count',), L2 sensitivity 1.000000\n",
      "Measuring ('VendorID',), L2 sensitivity 1.000000\n",
      "\n",
      "Measuring ('VendorID', 'extra'), L2 sensitivity 1.000000\n",
      "Measuring ('passenger_count', 'DOLocationID'), L2 sensitivity 1.000000\n",
      "Measuring ('RatecodeID', 'DOLocationID'), L2 sensitivity 1.000000\n",
      "Measuring ('store_and_fwd_flag', 'DOLocationID'), L2 sensitivity 1.000000\n",
      "Measuring ('PULocationID', 'DOLocationID'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'tip_amount'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'extra'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'congestion_surcharge'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'tolls_amount'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'airport_fee'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'mta_tax'), L2 sensitivity 1.000000\n",
      "Measuring ('DOLocationID', 'improvement_surcharge'), L2 sensitivity 1.000000\n",
      "\n",
      "Post-processing with Private-PGM, will take some time...\n",
      "Synthetic Data creation complete\n",
      "/home/shubham/private-pgm/src/mbi/__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n",
      "  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n",
      "improvement_surcharge  airport_fee              0.000491\n",
      "mta_tax                improvement_surcharge    0.000491\n",
      "                       airport_fee              0.000584\n",
      "RatecodeID             improvement_surcharge    0.001473\n",
      "store_and_fwd_flag     improvement_surcharge    0.001999\n",
      "                                                  ...   \n",
      "PULocationID           congestion_surcharge     0.226142\n",
      "                       airport_fee              0.241827\n",
      "                       tip_amount               0.248454\n",
      "                       extra                    0.249027\n",
      "                       DOLocationID             0.393212\n",
      "Length: 78, dtype: float64\n",
      "Average Error:  0.06883565102647118\n",
      "Scoring complete\n",
      "/home/shubham/private-pgm/src/mbi/__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n",
      "  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n",
      "Synthetic reconstruction complete\n"
     ]
    }
   ],
   "source": [
    "!cd extensions/\n",
    "\n",
    "!python extensions/transform.py --transform discretize --df yellow_cab_data/processed_train.csv --schema yellow_cab_data/parameters.json --output_dir df_transform_result/\n",
    "\n",
    "print(\"Transform complete\")\n",
    "\n",
    "discretized = pd.read_csv(os.path.join(\"df_transform_result\", \"discretized.csv\"))\n",
    "discretized = discretized.replace({-1: 9})\n",
    "discretized.to_csv(os.path.join(\"df_transform_result\", \"discretized.csv\"), index = False)\n",
    "\n",
    "!python extensions/check_domain.py --dataset df_transform_result/discretized.csv --domain df_transform_result/domain.json\n",
    "\n",
    "print(\"Domain check complete\")\n",
    "\n",
    "!python extensions/adaptive_grid.py --epsilon 0.1 --dataset df_transform_result/discretized.csv --domain df_transform_result/domain.json --save yellow_cab_data/yellow_cab_data_synthetic.csv\n",
    "\n",
    "print(\"Synthetic Data creation complete\")\n",
    "\n",
    "!python extensions/score.py --synthetic yellow_cab_data/yellow_cab_data_synthetic.csv --domain df_transform_result/domain.json --dataset df_transform_result/discretized.csv \n",
    "\n",
    "print(\"Scoring complete\")\n",
    "\n",
    "!python extensions/transform.py --transform undo_discretize --df yellow_cab_data/yellow_cab_data_synthetic.csv --schema yellow_cab_data/parameters.json --output_dir df_transform_result/\n",
    "\n",
    "print(\"Synthetic reconstruction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.8183143149088843\n",
      "MAE:  0.9300384550522129\n",
      "MAE:  0.9947814849123735\n",
      "MAE:  1.3097206347728323\n",
      "MAE:  1.1779841512193245\n",
      "MAE:  1.18013076193997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.6005104773447156, 1.2665972517805102, 1.1863215991036695)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reg_eval(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    print(\"MAE: \", mae)\n",
    "    # print(\"MSE: \", mse)\n",
    "    return mae\n",
    "\n",
    "original = pd.read_csv(os.path.join(\"yellow_cab_data\", \"processed_train.csv\"))\n",
    "\n",
    "synthetic = pd.read_csv(os.path.join(\"df_transform_result\", \"raw_synthetic.csv\"))\n",
    "\n",
    "X_train, y_train = original.drop('tip_amount', axis = 1), original['tip_amount']\n",
    "X_train_s, y_train_s = synthetic.drop('tip_amount', axis = 1), synthetic['tip_amount']\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1, n_jobs=-1)\n",
    "gb = GradientBoostingRegressor(random_state = 1)\n",
    "lr = LinearRegression()\n",
    "rf.fit(X_train, y_train)\n",
    "gb.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "rf_original_mae, gb_original_mae, lr_original_mae = reg_eval(rf, X_test, y_test), reg_eval(gb, X_test, y_test), reg_eval(lr, X_test, y_test)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1, n_jobs=-1)\n",
    "rf.fit(X_train_s, y_train_s)\n",
    "gb.fit(X_train_s, y_train_s)\n",
    "lr.fit(X_train_s, y_train_s)\n",
    "\n",
    "rf_dp_mae, gb_dp_mae, lr_dp_mae = reg_eval(rf, X_test, y_test), reg_eval(gb, X_test, y_test), reg_eval(lr, X_test, y_test)\n",
    "\n",
    "rf_dp_mae / rf_original_mae, gb_dp_mae / gb_original_mae, lr_dp_mae / lr_original_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce64d47c555e80c558740ea4991a544c6032f808535c22b0516a8288d661290f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
